{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Long Short-Term Memory with Pytorch using CPU and GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training data set, train=True\n",
    "# Make sure data is tesnor, transform=transforms.ToTensor()\n",
    "# download if doesn't exist, download=True\n",
    "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the testing dataset\n",
    "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "epoch_list = []\n",
    "for i in range(0,epochs):\n",
    "    epoch_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "isinstance(train_loader, collections.Iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "isinstance(test_loader, collections.Iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__ (self, input_size, hidden_size, layer_number, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Set hidden layer input size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Set number of hidden layer\n",
    "        self.layer_number = layer_number\n",
    "        \n",
    "        # Build RNN Unit\n",
    "        # batch_first = True means batch comes first in the new tensor shape\n",
    "        # so the tensor shape is (batch, seq_number, input_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, layer_number, batch_first = True)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initilize hidden state with 0\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            h0 = Variable(torch.zeros(self.layer_number, x.size(0), self.hidden_size).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(self.layer_number, x.size(0), self.hidden_size))\n",
    "            \n",
    "        # Initilize cell\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            c0 = Variable(torch.zeros(self.layer_number, x.size(0), self.hidden_size).cuda())\n",
    "        else:\n",
    "            c0 = Variable(torch.zeros(self.layer_number, x.size(0), self.hidden_size))\n",
    "            \n",
    "        # out size 100, 28, 100 (batch, seq_number, hidden_size)\n",
    "        out, hn = self.rnn(x, (h0,c0))\n",
    "        \n",
    "        # only use the last time step of the squence if the FC\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "hidden_size = 100\n",
    "layer_size = 3\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 5 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bdbd21423d01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create LSTM model instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 5 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "# Create LSTM model instance\n",
    "model = LSTMModel(input_size, hidden_size, layer_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to GPU if CUDA is available an the use_cuda is True\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the cost function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuateAccuracy():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "        \n",
    "    for images, labels in test_loader:\n",
    "        # get all the images in the training data set\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            # move torch tensors to the GPU\n",
    "            images = Variable(images.view(-1, seq_number, input_size).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images.view(-1, seq_number, input_size))\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        # get all the predictions for each class using the model\n",
    "        all_predictions = model(images)\n",
    "            \n",
    "        # highest is the final prediction\n",
    "        _, predictions = torch.max(all_predictions.data, 1)\n",
    "       \n",
    "        # total number of samples\n",
    "        total = total + labels.size(0)\n",
    "            \n",
    "        # corrrect prediction\n",
    "        # compare prediction with labels element wise\n",
    "        # sum returned tensor (it has True/1 or False/0 in it)\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            correct = correct + (predictions.cpu() == labels.cpu()).sum()\n",
    "        else:\n",
    "            correct = correct + (predictions == labels).sum()\n",
    "        \n",
    "        correct_np = correct.numpy()\n",
    "            \n",
    "    accuracy = 100 * correct_np / total;\n",
    "    print(\"accuracy: \" + str(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_number = 28\n",
    "\n",
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # get a batch of images and labels\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                #move torch tensors to the GPU\n",
    "                images = Variable(images.view(-1, seq_number, input_size).cuda())\n",
    "                labels = Variable(labels.cuda())\n",
    "            else:\n",
    "                images = Variable(images.view(-1, seq_number, input_size))\n",
    "                labels = Variable(labels)\n",
    "\n",
    "            # set gradients to 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get predictions for the batch\n",
    "            predictions = model(images)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_function(predictions, labels)\n",
    "\n",
    "            # calculate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "        accuracy = calcuateAccuracy()\n",
    "        accuracy_list.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    plt.clf()\n",
    "    plt.plot(epoch_list, accuracy_list, '-', alpha=0.5)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xticks(epoch_list) \n",
    "    plt.yticks(np.arange(min(accuracy_list)-min(accuracy_list)%5, max(accuracy_list), 5)) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
